{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unidecode\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import operator\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_model = \"en_core_web_lg\"\n",
    "\n",
    "tokenizer = re.compile(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveOBJ(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def loadOBJ(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing spaCy \"en_core_web_lg\"...\n",
      "Done!\n",
      "Time elapsed: 12s\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"Importing spaCy \\\"\"+spacy_model+\"\\\"...\")\n",
    "nlp = spacy.load(spacy_model)\n",
    "print(\"Done!\")\n",
    "print(\"Time elapsed: \"+str(round(time.time()-start))+\"s\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/train_data.csv\",index_col=\"id\")\n",
    "df_train = df_train.drop(\"is_duplicate\",axis=1)\n",
    "\n",
    "df_labels = pd.read_csv(\"data/train_labels.csv\",index_col=\"id\")\n",
    "\n",
    "df_train = df_train.join(df_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question1  \\\n",
       "id                                                      \n",
       "0   What is the step by step guide to invest in sh...   \n",
       "1   What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   How can I increase the speed of my internet co...   \n",
       "3   Why am I mentally very lonely? How can I solve...   \n",
       "4   Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                            question2  is_duplicate  \n",
       "id                                                                   \n",
       "0   What is the step by step guide to invest in sh...             0  \n",
       "1   What would happen if the Indian government sto...             0  \n",
       "2   How can Internet speed be increased by hacking...             0  \n",
       "3   Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4             Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction duplicate: 0.3688313054671931\n",
      "Fraction !duplicate: 0.6311686945328069\n"
     ]
    }
   ],
   "source": [
    "len0 = len(df_train[df_train[\"is_duplicate\"]==0])\n",
    "len1 = len(df_train[df_train[\"is_duplicate\"]==1])\n",
    "fraction = len1/(len0+len1)\n",
    "\n",
    "print('Fraction duplicate: %s' % (fraction))\n",
    "print('Fraction !duplicate: %s' % (1-fraction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_counts = {}\n",
    "\n",
    "# print(\"Starting with \"+str(len(df_train))+ \" rows..\")\n",
    "\n",
    "# for index, row in df_train.iterrows():\n",
    "#     docs = [row['question1'], row['question2']]\n",
    "#     for doc in docs:\n",
    "#         tokens = tokenizer.findall(str(doc).lower())\n",
    "#         for token in tokens:\n",
    "#             if token in token_counts:\n",
    "#                 token_counts[token] += 1\n",
    "#             else:\n",
    "#                 token_counts[token] = 1\n",
    "                \n",
    "#     if (index+1) % 10000 == 0:\n",
    "#         print(str(index+1)+\" rows done..\")\n",
    "\n",
    "# print()\n",
    "# print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_docs = 2*len(df_train)\n",
    "\n",
    "# token_idf = {}\n",
    "# for token in token_counts:\n",
    "#     token_idf[token] = np.log(n_docs/(token_counts[token]))\n",
    "    \n",
    "# saveOBJ(token_idf,\"data/token_IDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_idf = loadOBJ(\"data/token_IDF\")\n",
    "sorted_idf = sorted(token_idf.items(), key=operator.itemgetter(1),reverse=True)\n",
    "max_idf = sorted_idf[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "df_train = df_train[:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* first word same\n",
    "* last word same\n",
    "* length ratio\n",
    "* n_words ratio\n",
    "* context embedding similarity\n",
    "* token one-hot encoding similarity with TF-IDF\n",
    "* both contain (or don't contain) math\n",
    "* NER->both contain (or don't contain) same entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = tokenizer.findall(str(text).lower())\n",
    "    return tokens\n",
    "\n",
    "def firstWordSame(row):\n",
    "    if len(row['tokens1']) == 0 or len(row['tokens2']) == 0:\n",
    "        return 0\n",
    "    if row['tokens1'][0] == row['tokens2'][0]:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def lastWordSame(row):\n",
    "    if len(row['tokens1']) == 0 or len(row['tokens2']) == 0:\n",
    "        return 0\n",
    "    if row['tokens1'][-1] == row['tokens2'][-1]:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def embedding_similarity(row):\n",
    "    if len(row['tokens1']) == 0 or len(row['tokens2']) == 0:\n",
    "        return 0\n",
    "    docs = []\n",
    "    docs.append(nlp(\" \".join(row['tokens1'])))\n",
    "    docs.append(nlp(\" \".join(row['tokens2'])))\n",
    "    vectors = []\n",
    "    for doc in docs:    \n",
    "        subvectors = []\n",
    "        for token in doc:\n",
    "            if not token.has_vector:\n",
    "                continue\n",
    "            subvectors.append(token.vector)\n",
    "        if len(subvectors) == 0:\n",
    "            return 0\n",
    "        subvectors = np.array(subvectors)\n",
    "        vector = np.average(subvectors,axis=0)\n",
    "        if np.linalg.norm(vector) == 0:\n",
    "            return 0\n",
    "        vectors.append(vector)\n",
    "    similarity = np.dot(vectors[0], vectors[1])/(np.linalg.norm(vectors[0])*np.linalg.norm(vectors[1]))\n",
    "    return similarity\n",
    "\n",
    "def embedding_similarity2(row):\n",
    "    if len(row['tokens1']) == 0 or len(row['tokens2']) == 0:\n",
    "        return 0\n",
    "\n",
    "    token_sets = [row['tokens1'],row['tokens2']]\n",
    "    vectors = []\n",
    "    for token_set in token_sets:    \n",
    "        subvectors = []\n",
    "        for token in token_set:\n",
    "            token_id = nlp.vocab.strings[token]\n",
    "            try:\n",
    "                subvectors.append(nlp.vocab.vectors[token_id])\n",
    "            except:\n",
    "                continue\n",
    "        if len(subvectors) == 0:\n",
    "            return 0\n",
    "        subvectors = np.array(subvectors)\n",
    "        vector = np.average(subvectors,axis=0)\n",
    "        if np.linalg.norm(vector) == 0:\n",
    "            return 0\n",
    "        vectors.append(vector)\n",
    "    similarity = np.dot(vectors[0], vectors[1])/(np.linalg.norm(vectors[0])*np.linalg.norm(vectors[1]))\n",
    "    return similarity\n",
    "\n",
    "def word_ratio(row):\n",
    "    if len(row['tokens1']) == 0 or len(row['tokens2']) == 0:\n",
    "        return 0\n",
    "    ratio = len(row['tokens1'])/len(row['tokens2'])\n",
    "    if ratio > 1:\n",
    "        return 1/ratio\n",
    "    return ratio\n",
    "\n",
    "def char_ratio(row):\n",
    "    if len(row['tokens1']) == 0 or len(row['tokens2']) == 0:\n",
    "        return 0\n",
    "    ratio = len(\"\".join(row['tokens1']))/len(\"\".join(row['tokens2']))\n",
    "    if ratio > 1:\n",
    "        return 1/ratio\n",
    "    return ratio\n",
    "\n",
    "def math_similarity(row):\n",
    "    hit1 = 0\n",
    "    hit2 = 0\n",
    "    if \"[math]\" in str(row[\"question1\"]):\n",
    "        hit1 = 1\n",
    "    if \"[math]\" in str(row[\"question2\"]):\n",
    "        hit2 = 1\n",
    "    if hit1 == hit2:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def getVector(tokens,TFIDF=False):\n",
    "    if len(tokens) == 0:\n",
    "        return 0\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        token_id = nlp.vocab.strings[token]\n",
    "        try:\n",
    "            weight = 1\n",
    "            if TFIDF:\n",
    "                if token in token_idf:\n",
    "                    weight = token_idf[token]\n",
    "                else:\n",
    "                    weight = max_idf\n",
    "            vectors.append(nlp.vocab.vectors[token_id]*weight)\n",
    "        except:\n",
    "            continue\n",
    "    if len(vectors) == 0:\n",
    "        return 0\n",
    "    vectors = np.array(vectors)\n",
    "    vector = np.average(vectors,axis=0)\n",
    "    if np.linalg.norm(vector) == 0:\n",
    "        return 0\n",
    "    return vector\n",
    "\n",
    "def cosine_similarity(vectors):\n",
    "    v1 = vectors[0]\n",
    "    v2 = vectors[1]\n",
    "    if np.linalg.norm(v1) == 0:\n",
    "        return 0\n",
    "    elif np.linalg.norm(v2) == 0:\n",
    "        return 0\n",
    "    similarity = np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def multiply_vectors(vectors):\n",
    "    v1 = vectors[0]\n",
    "    v2 = vectors[1]\n",
    "    if np.linalg.norm(v1) == 0:\n",
    "        return np.zeros(300)\n",
    "    elif np.linalg.norm(v2) == 0:\n",
    "        return np.zeros(300)\n",
    "    product = v1*v2\n",
    "    return product\n",
    "\n",
    "def add_vectors(vectors):\n",
    "    v1 = vectors[0]\n",
    "    v2 = vectors[1]\n",
    "    if np.linalg.norm(v1) == 0:\n",
    "        return np.zeros(300)\n",
    "    elif np.linalg.norm(v2) == 0:\n",
    "        return np.zeros(300)\n",
    "    added = np.absolute(v1+v2)\n",
    "    return added\n",
    "\n",
    "def token_similarity(row):\n",
    "    if len(row['tokens1']) == 0 or len(row['tokens2']) == 0:\n",
    "        return 0\n",
    "    \n",
    "    #tokens of each question, including repeated words\n",
    "    token_sets = [row['tokens1'],row['tokens2']]\n",
    "    \n",
    "    #total unique tokens across both questions\n",
    "    total_tokens = list(set(row['tokens1']+row['tokens2']))\n",
    "    \n",
    "    vectors = []\n",
    "    for token_set in token_sets:\n",
    "        token_vector = np.zeros(len(total_tokens))\n",
    "        for token in token_set:\n",
    "            if token in token_idf:\n",
    "                #use += instead of = to automatically include TF\n",
    "                token_vector[total_tokens.index(token)] += token_idf[token]\n",
    "            else:\n",
    "                token_vector[total_tokens.index(token)] += max_idf\n",
    "        if np.linalg.norm(token_vector) == 0:\n",
    "            return 0\n",
    "        token_vector = token_vector/np.linalg.norm(token_vector)\n",
    "        vectors.append(token_vector)\n",
    "\n",
    "    #only dot product is sufficient because already normalized\n",
    "    similarity = np.dot(vectors[0],vectors[1])\n",
    "    return similarity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:01<00:00, 88091.68it/s]\n",
      "100%|██████████| 100000/100000 [00:00<00:00, 109725.43it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['tokens1'] = df_train['question1'].progress_apply(lambda x: tokenize(x))\n",
    "df_train['tokens2'] = df_train['question2'].progress_apply(lambda x: tokenize(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:11<00:00, 8689.21it/s]\n",
      "100%|██████████| 100000/100000 [00:10<00:00, 9395.74it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['vector1'] = df_train['tokens1'].progress_apply(lambda x: getVector(x))\n",
    "df_train['vector2'] = df_train['tokens2'].progress_apply(lambda x: getVector(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:12<00:00, 7843.21it/s]\n",
      "100%|██████████| 100000/100000 [00:12<00:00, 7822.25it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['vector_tfidf1'] = df_train['tokens1'].progress_apply(lambda x: getVector(x,TFIDF=True))\n",
    "df_train['vector_tfidf2'] = df_train['tokens2'].progress_apply(lambda x: getVector(x,TFIDF=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:12<00:00, 7914.04it/s]\n",
      "100%|██████████| 100000/100000 [00:12<00:00, 8035.01it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['embedding_similarity'] = df_train[['vector1','vector2']].progress_apply(lambda row: cosine_similarity(row),axis=1)\n",
    "df_train['embedding_similarity_tfidf'] = df_train[['vector_tfidf1','vector_tfidf2']].progress_apply(lambda row: cosine_similarity(row),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:18<00:00, 5297.34it/s]\n",
      "100%|██████████| 100000/100000 [00:18<00:00, 5469.09it/s]\n",
      "100%|██████████| 100000/100000 [00:17<00:00, 5696.39it/s]\n",
      "100%|██████████| 100000/100000 [00:20<00:00, 4951.55it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['vector_combo1'] = df_train[['vector1','vector2']].progress_apply(lambda row: multiply_vectors(row),axis=1)\n",
    "df_train['vector_combo2'] = df_train[['vector1','vector2']].progress_apply(lambda row: add_vectors(row),axis=1)\n",
    "df_train['vector_combo3'] = df_train[['vector_tfidf1','vector_tfidf2']].progress_apply(lambda row: multiply_vectors(row),axis=1)\n",
    "df_train['vector_combo4'] = df_train[['vector_tfidf1','vector_tfidf2']].progress_apply(lambda row: add_vectors(row),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:05<00:00, 17506.10it/s]\n",
      "100%|██████████| 100000/100000 [00:05<00:00, 17839.72it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['firstWordSame'] = df_train[['tokens1','tokens2']].progress_apply(lambda row: firstWordSame(row),axis=1)\n",
    "df_train['lastWordSame'] = df_train[['tokens1','tokens2']].progress_apply(lambda row: lastWordSame(row),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:05<00:00, 16835.56it/s]\n",
      "100%|██████████| 100000/100000 [00:06<00:00, 16648.21it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['word_ratio'] = df_train[['tokens1','tokens2']].progress_apply(lambda row: word_ratio(row),axis=1)\n",
    "df_train['char_ratio'] = df_train[['tokens1','tokens2']].progress_apply(lambda row: char_ratio(row),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100000 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 3335/100000 [00:00<00:02, 33347.57it/s]\u001b[A\n",
      "  5%|▍         | 4826/100000 [00:00<00:03, 24321.45it/s]\u001b[A\n",
      "  6%|▌         | 5974/100000 [00:00<00:05, 16161.27it/s]\u001b[A\n",
      "  8%|▊         | 7512/100000 [00:00<00:05, 15908.61it/s]\u001b[A\n",
      " 10%|▉         | 9935/100000 [00:00<00:05, 17735.94it/s]\u001b[A\n",
      " 12%|█▏        | 11807/100000 [00:00<00:04, 18011.60it/s]\u001b[A\n",
      " 14%|█▎        | 13577/100000 [00:00<00:04, 17915.63it/s]\u001b[A\n",
      " 16%|█▌        | 15733/100000 [00:00<00:04, 18856.81it/s]\u001b[A\n",
      " 18%|█▊        | 17829/100000 [00:00<00:04, 19432.73it/s]\u001b[A\n",
      " 20%|██        | 20197/100000 [00:01<00:03, 20536.71it/s]\u001b[A\n",
      " 22%|██▏       | 22476/100000 [00:01<00:03, 21075.94it/s]\u001b[A\n",
      " 25%|██▍       | 24583/100000 [00:01<00:04, 17454.49it/s]\u001b[A\n",
      " 26%|██▋       | 26432/100000 [00:01<00:04, 14811.57it/s]\u001b[A\n",
      " 29%|██▉       | 28770/100000 [00:01<00:04, 16621.24it/s]\u001b[A\n",
      " 31%|███       | 30604/100000 [00:01<00:04, 15235.62it/s]\u001b[A\n",
      " 32%|███▏      | 32270/100000 [00:01<00:04, 14019.51it/s]\u001b[A\n",
      " 34%|███▍      | 34048/100000 [00:01<00:04, 14968.17it/s]\u001b[A\n",
      " 36%|███▌      | 36173/100000 [00:02<00:03, 16424.68it/s]\u001b[A\n",
      " 39%|███▊      | 38605/100000 [00:02<00:03, 18196.69it/s]\u001b[A\n",
      " 42%|████▏     | 41563/100000 [00:02<00:02, 20571.21it/s]\u001b[A\n",
      " 44%|████▍     | 44011/100000 [00:02<00:02, 21604.63it/s]\u001b[A\n",
      " 46%|████▋     | 46496/100000 [00:02<00:02, 22471.53it/s]\u001b[A\n",
      " 49%|████▉     | 49298/100000 [00:02<00:02, 23889.69it/s]\u001b[A\n",
      " 52%|█████▏    | 51880/100000 [00:02<00:01, 24433.54it/s]\u001b[A\n",
      " 55%|█████▍    | 54656/100000 [00:02<00:01, 25343.61it/s]\u001b[A\n",
      " 57%|█████▋    | 57254/100000 [00:02<00:01, 25455.35it/s]\u001b[A\n",
      " 60%|█████▉    | 59844/100000 [00:02<00:01, 24651.93it/s]\u001b[A\n",
      " 62%|██████▏   | 62346/100000 [00:03<00:02, 16840.45it/s]\u001b[A\n",
      " 64%|██████▍   | 64388/100000 [00:03<00:02, 17700.75it/s]\u001b[A\n",
      " 67%|██████▋   | 66799/100000 [00:03<00:01, 19233.81it/s]\u001b[A\n",
      " 69%|██████▉   | 69405/100000 [00:03<00:01, 20873.29it/s]\u001b[A\n",
      " 72%|███████▏  | 71992/100000 [00:03<00:01, 22157.00it/s]\u001b[A\n",
      " 75%|███████▍  | 74718/100000 [00:03<00:01, 23471.60it/s]\u001b[A\n",
      " 77%|███████▋  | 77308/100000 [00:03<00:00, 24139.66it/s]\u001b[A\n",
      " 80%|███████▉  | 79820/100000 [00:03<00:00, 24187.77it/s]\u001b[A\n",
      " 82%|████████▏ | 82394/100000 [00:04<00:00, 24630.53it/s]\u001b[A\n",
      " 85%|████████▍ | 84907/100000 [00:04<00:00, 24728.54it/s]\u001b[A\n",
      " 87%|████████▋ | 87415/100000 [00:04<00:00, 24021.50it/s]\u001b[A\n",
      " 90%|████████▉ | 89846/100000 [00:04<00:00, 22588.90it/s]\u001b[A\n",
      " 92%|█████████▏| 92143/100000 [00:04<00:00, 17080.56it/s]\u001b[A\n",
      " 95%|█████████▍| 94737/100000 [00:04<00:00, 19000.40it/s]\u001b[A\n",
      " 98%|█████████▊| 97783/100000 [00:04<00:00, 21416.36it/s]\u001b[A\n",
      "100%|██████████| 100000/100000 [00:04<00:00, 20467.59it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "df_train['math_similarity'] = df_train[['question1','question2']].progress_apply(lambda row: math_similarity(row),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100000 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 289/100000 [00:00<00:34, 2888.41it/s]\u001b[A\n",
      "  1%|          | 578/100000 [00:00<00:34, 2888.01it/s]\u001b[A\n",
      "  1%|          | 799/100000 [00:00<00:37, 2640.11it/s]\u001b[A\n",
      "  1%|          | 1000/100000 [00:00<00:41, 2411.54it/s]\u001b[A\n",
      "  1%|▏         | 1279/100000 [00:00<00:39, 2512.27it/s]\u001b[A\n",
      "  2%|▏         | 1531/100000 [00:00<00:39, 2513.56it/s]\u001b[A\n",
      "  2%|▏         | 1798/100000 [00:00<00:38, 2558.13it/s]\u001b[A\n",
      "  2%|▏         | 2034/100000 [00:00<00:40, 2429.36it/s]\u001b[A\n",
      "  2%|▏         | 2265/100000 [00:00<00:40, 2391.26it/s]\u001b[A\n",
      "  2%|▏         | 2496/100000 [00:01<00:42, 2301.16it/s]\u001b[A\n",
      "  3%|▎         | 2742/100000 [00:01<00:41, 2342.68it/s]\u001b[A\n",
      "  3%|▎         | 2994/100000 [00:01<00:40, 2393.03it/s]\u001b[A\n",
      "  3%|▎         | 3231/100000 [00:01<00:41, 2345.91it/s]\u001b[A\n",
      "  4%|▎         | 3542/100000 [00:01<00:38, 2529.21it/s]\u001b[A\n",
      "  4%|▍         | 3879/100000 [00:01<00:35, 2729.37it/s]\u001b[A\n",
      "  4%|▍         | 4224/100000 [00:01<00:32, 2911.33it/s]\u001b[A\n",
      "  5%|▍         | 4582/100000 [00:01<00:30, 3082.43it/s]\u001b[A\n",
      "  5%|▍         | 4961/100000 [00:01<00:29, 3259.61it/s]\u001b[A\n",
      "  5%|▌         | 5299/100000 [00:01<00:28, 3289.58it/s]\u001b[A\n",
      "  6%|▌         | 5635/100000 [00:02<00:28, 3305.44it/s]\u001b[A\n",
      "  6%|▌         | 6000/100000 [00:02<00:27, 3399.35it/s]\u001b[A\n",
      "  6%|▋         | 6344/100000 [00:02<00:27, 3387.76it/s]\u001b[A\n",
      "  7%|▋         | 6686/100000 [00:02<00:29, 3209.24it/s]\u001b[A\n",
      "  7%|▋         | 7013/100000 [00:02<00:28, 3227.20it/s]\u001b[A\n",
      "  7%|▋         | 7400/100000 [00:02<00:27, 3395.85it/s]\u001b[A\n",
      "  8%|▊         | 7744/100000 [00:02<00:28, 3257.42it/s]\u001b[A\n",
      "  8%|▊         | 8074/100000 [00:02<00:28, 3181.45it/s]\u001b[A\n",
      "  8%|▊         | 8396/100000 [00:02<00:31, 2930.00it/s]\u001b[A\n",
      "  9%|▊         | 8700/100000 [00:03<00:31, 2942.83it/s]\u001b[A\n",
      "  9%|▉         | 8999/100000 [00:03<00:31, 2846.24it/s]\u001b[A\n",
      "  9%|▉         | 9288/100000 [00:03<00:34, 2650.12it/s]\u001b[A\n",
      " 10%|▉         | 9559/100000 [00:03<00:35, 2512.37it/s]\u001b[A\n",
      " 10%|▉         | 9816/100000 [00:03<00:36, 2465.39it/s]\u001b[A\n",
      " 10%|█         | 10067/100000 [00:03<00:42, 2098.11it/s]\u001b[A\n",
      " 10%|█         | 10289/100000 [00:03<00:43, 2053.01it/s]\u001b[A\n",
      " 11%|█         | 10504/100000 [00:03<00:47, 1894.34it/s]\u001b[A\n",
      " 11%|█         | 10702/100000 [00:03<00:47, 1884.77it/s]\u001b[A\n",
      " 11%|█         | 10926/100000 [00:04<00:45, 1977.66it/s]\u001b[A\n",
      " 11%|█         | 11152/100000 [00:04<00:43, 2050.30it/s]\u001b[A\n",
      " 11%|█▏        | 11362/100000 [00:04<00:44, 1978.65it/s]\u001b[A\n",
      " 12%|█▏        | 11564/100000 [00:04<00:47, 1877.57it/s]\u001b[A\n",
      " 12%|█▏        | 11783/100000 [00:04<00:45, 1944.73it/s]\u001b[A\n",
      " 12%|█▏        | 12047/100000 [00:04<00:41, 2100.17it/s]\u001b[A\n",
      " 12%|█▏        | 12324/100000 [00:04<00:38, 2262.49it/s]\u001b[A\n",
      " 13%|█▎        | 12567/100000 [00:04<00:37, 2309.64it/s]\u001b[A\n",
      " 13%|█▎        | 12843/100000 [00:04<00:35, 2428.03it/s]\u001b[A\n",
      " 13%|█▎        | 13092/100000 [00:05<00:49, 1762.78it/s]\u001b[A\n",
      " 13%|█▎        | 13299/100000 [00:05<00:50, 1723.84it/s]\u001b[A\n",
      " 13%|█▎        | 13495/100000 [00:05<00:48, 1787.72it/s]\u001b[A\n",
      " 14%|█▎        | 13723/100000 [00:05<00:45, 1909.57it/s]\u001b[A\n",
      " 14%|█▍        | 14013/100000 [00:05<00:40, 2127.24it/s]\u001b[A\n",
      " 14%|█▍        | 14291/100000 [00:05<00:37, 2288.27it/s]\u001b[A\n",
      " 15%|█▍        | 14536/100000 [00:05<00:38, 2225.09it/s]\u001b[A\n",
      " 15%|█▍        | 14807/100000 [00:05<00:36, 2348.11it/s]\u001b[A\n",
      " 15%|█▌        | 15052/100000 [00:06<00:37, 2292.18it/s]\u001b[A\n",
      " 15%|█▌        | 15289/100000 [00:06<00:41, 2054.84it/s]\u001b[A\n",
      " 16%|█▌        | 15521/100000 [00:06<00:39, 2126.17it/s]\u001b[A\n",
      " 16%|█▌        | 15920/100000 [00:06<00:34, 2472.09it/s]\u001b[A\n",
      " 16%|█▋        | 16269/100000 [00:06<00:30, 2708.45it/s]\u001b[A\n",
      " 17%|█▋        | 16662/100000 [00:06<00:27, 2986.84it/s]\u001b[A\n",
      " 17%|█▋        | 17019/100000 [00:06<00:26, 3140.60it/s]\u001b[A\n",
      " 18%|█▊        | 17522/100000 [00:06<00:23, 3539.11it/s]\u001b[A\n",
      " 18%|█▊        | 17908/100000 [00:06<00:22, 3616.54it/s]\u001b[A\n",
      " 18%|█▊        | 18292/100000 [00:06<00:22, 3610.24it/s]\u001b[A\n",
      " 19%|█▊        | 18669/100000 [00:07<00:22, 3627.81it/s]\u001b[A\n",
      " 19%|█▉        | 19043/100000 [00:07<00:23, 3397.10it/s]\u001b[A\n",
      " 19%|█▉        | 19416/100000 [00:07<00:23, 3488.73it/s]\u001b[A\n",
      " 20%|█▉        | 19773/100000 [00:07<00:24, 3313.49it/s]\u001b[A\n",
      " 20%|██        | 20132/100000 [00:07<00:23, 3385.04it/s]\u001b[A\n",
      " 21%|██        | 20552/100000 [00:07<00:22, 3593.06it/s]\u001b[A\n",
      " 21%|██        | 20919/100000 [00:07<00:23, 3376.23it/s]\u001b[A\n",
      " 21%|██▏       | 21299/100000 [00:07<00:22, 3493.09it/s]\u001b[A\n",
      " 22%|██▏       | 21721/100000 [00:07<00:21, 3682.47it/s]\u001b[A\n",
      " 22%|██▏       | 22097/100000 [00:08<00:22, 3534.92it/s]\u001b[A\n",
      " 22%|██▏       | 22457/100000 [00:08<00:22, 3473.97it/s]\u001b[A\n",
      " 23%|██▎       | 22810/100000 [00:08<00:27, 2820.93it/s]\u001b[A\n",
      " 23%|██▎       | 23131/100000 [00:08<00:26, 2923.94it/s]\u001b[A\n",
      " 23%|██▎       | 23441/100000 [00:08<00:26, 2862.32it/s]\u001b[A\n",
      " 24%|██▎       | 23740/100000 [00:08<00:26, 2898.89it/s]\u001b[A\n",
      " 24%|██▍       | 24106/100000 [00:08<00:24, 3091.48it/s]\u001b[A\n",
      " 24%|██▍       | 24457/100000 [00:08<00:23, 3205.26it/s]\u001b[A\n",
      " 25%|██▍       | 24818/100000 [00:08<00:22, 3302.63it/s]\u001b[A\n",
      " 25%|██▌       | 25155/100000 [00:09<00:23, 3188.30it/s]\u001b[A\n",
      " 25%|██▌       | 25480/100000 [00:09<00:24, 3048.99it/s]\u001b[A\n",
      " 26%|██▌       | 25790/100000 [00:09<00:25, 2878.51it/s]\u001b[A\n",
      " 26%|██▌       | 26084/100000 [00:09<00:27, 2673.04it/s]\u001b[A\n",
      " 26%|██▋       | 26359/100000 [00:09<00:29, 2537.81it/s]\u001b[A\n",
      " 27%|██▋       | 26691/100000 [00:09<00:26, 2729.74it/s]\u001b[A\n",
      " 27%|██▋       | 27068/100000 [00:09<00:24, 2974.55it/s]\u001b[A\n",
      " 28%|██▊       | 27530/100000 [00:09<00:21, 3330.39it/s]\u001b[A\n",
      " 28%|██▊       | 27928/100000 [00:09<00:20, 3501.29it/s]\u001b[A\n",
      " 28%|██▊       | 28297/100000 [00:10<00:20, 3543.29it/s]\u001b[A\n",
      " 29%|██▊       | 28665/100000 [00:10<00:22, 3114.71it/s]\u001b[A\n",
      " 29%|██▉       | 28995/100000 [00:10<00:24, 2898.51it/s]\u001b[A\n",
      " 29%|██▉       | 29301/100000 [00:10<00:24, 2841.50it/s]\u001b[A\n",
      " 30%|██▉       | 29597/100000 [00:10<00:26, 2672.15it/s]\u001b[A\n",
      " 30%|██▉       | 29944/100000 [00:10<00:24, 2869.03it/s]\u001b[A\n",
      " 30%|███       | 30312/100000 [00:10<00:22, 3069.09it/s]\u001b[A\n",
      " 31%|███       | 30641/100000 [00:10<00:22, 3130.80it/s]\u001b[A\n",
      " 31%|███       | 30968/100000 [00:11<00:21, 3165.77it/s]\u001b[A\n",
      " 31%|███▏      | 31291/100000 [00:11<00:22, 3044.04it/s]\u001b[A\n",
      " 32%|███▏      | 31601/100000 [00:11<00:23, 2959.85it/s]\u001b[A\n",
      " 32%|███▏      | 31902/100000 [00:11<00:23, 2953.74it/s]\u001b[A\n",
      " 32%|███▏      | 32201/100000 [00:11<00:23, 2852.65it/s]\u001b[A\n",
      " 32%|███▏      | 32490/100000 [00:11<00:26, 2577.13it/s]\u001b[A\n",
      " 33%|███▎      | 32755/100000 [00:11<00:26, 2529.48it/s]\u001b[A\n",
      " 33%|███▎      | 33115/100000 [00:11<00:24, 2776.77it/s]\u001b[A\n",
      " 34%|███▎      | 33538/100000 [00:11<00:21, 3095.04it/s]\u001b[A\n",
      " 34%|███▍      | 33893/100000 [00:11<00:20, 3217.93it/s]\u001b[A\n",
      " 34%|███▍      | 34273/100000 [00:12<00:19, 3371.10it/s]\u001b[A\n",
      " 35%|███▍      | 34623/100000 [00:12<00:20, 3249.67it/s]\u001b[A\n",
      " 35%|███▍      | 34958/100000 [00:12<00:20, 3128.93it/s]\u001b[A\n",
      " 35%|███▌      | 35279/100000 [00:12<00:25, 2557.21it/s]\u001b[A\n",
      " 36%|███▌      | 35699/100000 [00:12<00:22, 2896.86it/s]\u001b[A\n",
      " 36%|███▌      | 36115/100000 [00:12<00:20, 3186.43it/s]\u001b[A\n",
      " 36%|███▋      | 36465/100000 [00:12<00:20, 3151.91it/s]\u001b[A\n",
      " 37%|███▋      | 36831/100000 [00:12<00:19, 3288.41it/s]\u001b[A\n",
      " 37%|███▋      | 37346/100000 [00:13<00:16, 3688.10it/s]\u001b[A\n",
      " 38%|███▊      | 37875/100000 [00:13<00:15, 4056.30it/s]\u001b[A\n",
      " 38%|███▊      | 38383/100000 [00:13<00:14, 4315.63it/s]\u001b[A\n",
      " 39%|███▉      | 38896/100000 [00:13<00:13, 4529.95it/s]\u001b[A\n",
      " 39%|███▉      | 39370/100000 [00:13<00:14, 4241.01it/s]\u001b[A\n",
      " 40%|███▉      | 39813/100000 [00:13<00:15, 3981.30it/s]\u001b[A\n",
      " 40%|████      | 40228/100000 [00:13<00:16, 3701.34it/s]\u001b[A\n",
      " 41%|████      | 40614/100000 [00:13<00:16, 3613.66it/s]\u001b[A\n",
      " 41%|████      | 40987/100000 [00:13<00:16, 3515.61it/s]\u001b[A\n",
      " 41%|████▏     | 41417/100000 [00:14<00:15, 3717.54it/s]\u001b[A\n",
      " 42%|████▏     | 41798/100000 [00:14<00:16, 3482.07it/s]\u001b[A\n",
      " 42%|████▏     | 42156/100000 [00:14<00:17, 3235.38it/s]\u001b[A\n",
      " 42%|████▏     | 42490/100000 [00:14<00:18, 3124.48it/s]\u001b[A\n",
      " 43%|████▎     | 42811/100000 [00:14<00:20, 2851.30it/s]\u001b[A\n",
      " 43%|████▎     | 43107/100000 [00:14<00:20, 2731.44it/s]\u001b[A\n",
      " 43%|████▎     | 43389/100000 [00:14<00:22, 2512.43it/s]\u001b[A\n",
      " 44%|████▎     | 43650/100000 [00:14<00:24, 2293.89it/s]\u001b[A\n",
      " 44%|████▍     | 43890/100000 [00:15<00:27, 2029.00it/s]\u001b[A\n",
      " 44%|████▍     | 44106/100000 [00:15<00:33, 1689.69it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44335/100000 [00:15<00:30, 1832.47it/s]\u001b[A\n",
      " 45%|████▍     | 44630/100000 [00:15<00:26, 2062.84it/s]\u001b[A\n",
      " 45%|████▍     | 44889/100000 [00:15<00:25, 2196.53it/s]\u001b[A\n",
      " 45%|████▌     | 45126/100000 [00:15<00:28, 1957.53it/s]\u001b[A\n",
      " 45%|████▌     | 45372/100000 [00:15<00:26, 2084.55it/s]\u001b[A\n",
      " 46%|████▌     | 45631/100000 [00:15<00:24, 2213.31it/s]\u001b[A\n",
      " 46%|████▌     | 45932/100000 [00:16<00:22, 2404.16it/s]\u001b[A\n",
      " 46%|████▌     | 46186/100000 [00:16<00:23, 2330.84it/s]\u001b[A\n",
      " 46%|████▋     | 46429/100000 [00:16<00:22, 2337.00it/s]\u001b[A\n",
      " 47%|████▋     | 46670/100000 [00:16<00:26, 2034.62it/s]\u001b[A\n",
      " 47%|████▋     | 46916/100000 [00:16<00:24, 2142.08it/s]\u001b[A\n",
      " 47%|████▋     | 47140/100000 [00:16<00:27, 1919.42it/s]\u001b[A\n",
      " 47%|████▋     | 47395/100000 [00:16<00:25, 2073.20it/s]\u001b[A\n",
      " 48%|████▊     | 47680/100000 [00:16<00:23, 2257.23it/s]\u001b[A\n",
      " 48%|████▊     | 47979/100000 [00:16<00:21, 2433.49it/s]\u001b[A\n",
      " 48%|████▊     | 48236/100000 [00:17<00:22, 2282.17it/s]\u001b[A\n",
      " 48%|████▊     | 48476/100000 [00:17<00:22, 2288.98it/s]\u001b[A\n",
      " 49%|████▊     | 48717/100000 [00:17<00:22, 2322.10it/s]\u001b[A\n",
      " 49%|████▉     | 49036/100000 [00:17<00:20, 2528.15it/s]\u001b[A\n",
      " 49%|████▉     | 49299/100000 [00:17<00:22, 2250.93it/s]\u001b[A\n",
      " 50%|████▉     | 49566/100000 [00:17<00:21, 2359.81it/s]\u001b[A\n",
      " 50%|████▉     | 49878/100000 [00:17<00:19, 2544.89it/s]\u001b[A\n",
      " 50%|█████     | 50207/100000 [00:17<00:18, 2723.08it/s]\u001b[A\n",
      " 51%|█████     | 50579/100000 [00:17<00:16, 2960.89it/s]\u001b[A\n",
      " 51%|█████     | 50890/100000 [00:18<00:17, 2840.49it/s]\u001b[A\n",
      " 51%|█████     | 51185/100000 [00:18<00:25, 1946.42it/s]\u001b[A\n",
      " 51%|█████▏    | 51426/100000 [00:18<00:27, 1763.32it/s]\u001b[A\n",
      " 52%|█████▏    | 51753/100000 [00:18<00:23, 2045.71it/s]\u001b[A\n",
      " 52%|█████▏    | 52211/100000 [00:18<00:19, 2452.34it/s]\u001b[A\n",
      " 53%|█████▎    | 52697/100000 [00:18<00:16, 2879.93it/s]\u001b[A\n",
      " 53%|█████▎    | 53193/100000 [00:18<00:14, 3294.27it/s]\u001b[A\n",
      " 54%|█████▎    | 53689/100000 [00:18<00:12, 3662.51it/s]\u001b[A\n",
      " 54%|█████▍    | 54182/100000 [00:19<00:11, 3968.45it/s]\u001b[A\n",
      " 55%|█████▍    | 54632/100000 [00:19<00:11, 4022.40it/s]\u001b[A\n",
      " 55%|█████▌    | 55071/100000 [00:19<00:11, 3925.25it/s]\u001b[A\n",
      " 55%|█████▌    | 55490/100000 [00:19<00:11, 3937.32it/s]\u001b[A\n",
      " 56%|█████▌    | 55906/100000 [00:19<00:11, 3998.23it/s]\u001b[A\n",
      " 56%|█████▋    | 56432/100000 [00:19<00:10, 4307.97it/s]\u001b[A\n",
      " 57%|█████▋    | 56924/100000 [00:19<00:09, 4473.71it/s]\u001b[A\n",
      " 57%|█████▋    | 57404/100000 [00:19<00:09, 4565.61it/s]\u001b[A\n",
      " 58%|█████▊    | 57889/100000 [00:19<00:09, 4645.80it/s]\u001b[A\n",
      " 58%|█████▊    | 58384/100000 [00:19<00:08, 4731.25it/s]\u001b[A\n",
      " 59%|█████▉    | 58863/100000 [00:20<00:08, 4701.08it/s]\u001b[A\n",
      " 59%|█████▉    | 59368/100000 [00:20<00:08, 4800.26it/s]\u001b[A\n",
      " 60%|█████▉    | 59852/100000 [00:20<00:08, 4658.22it/s]\u001b[A\n",
      " 60%|██████    | 60321/100000 [00:20<00:08, 4533.65it/s]\u001b[A\n",
      " 61%|██████    | 60778/100000 [00:20<00:08, 4451.06it/s]\u001b[A\n",
      " 61%|██████▏   | 61250/100000 [00:20<00:08, 4526.09it/s]\u001b[A\n",
      " 62%|██████▏   | 61705/100000 [00:20<00:09, 4188.93it/s]\u001b[A\n",
      " 62%|██████▏   | 62131/100000 [00:20<00:09, 3948.59it/s]\u001b[A\n",
      " 63%|██████▎   | 62552/100000 [00:20<00:09, 4022.77it/s]\u001b[A\n",
      " 63%|██████▎   | 63025/100000 [00:21<00:08, 4210.97it/s]\u001b[A\n",
      " 64%|██████▎   | 63535/100000 [00:21<00:08, 4443.20it/s]\u001b[A\n",
      " 64%|██████▍   | 64030/100000 [00:21<00:07, 4582.85it/s]\u001b[A\n",
      " 64%|██████▍   | 64495/100000 [00:21<00:07, 4560.73it/s]\u001b[A\n",
      " 65%|██████▍   | 64983/100000 [00:21<00:07, 4650.00it/s]\u001b[A\n",
      " 65%|██████▌   | 65452/100000 [00:21<00:08, 3999.06it/s]\u001b[A\n",
      " 66%|██████▌   | 65966/100000 [00:21<00:07, 4282.85it/s]\u001b[A\n",
      " 67%|██████▋   | 66539/100000 [00:21<00:07, 4632.45it/s]\u001b[A\n",
      " 67%|██████▋   | 67032/100000 [00:21<00:06, 4717.84it/s]\u001b[A\n",
      " 68%|██████▊   | 67520/100000 [00:22<00:07, 4448.82it/s]\u001b[A\n",
      " 68%|██████▊   | 67979/100000 [00:22<00:08, 3977.95it/s]\u001b[A\n",
      " 68%|██████▊   | 68396/100000 [00:22<00:08, 3626.94it/s]\u001b[A\n",
      " 69%|██████▉   | 68824/100000 [00:22<00:08, 3799.84it/s]\u001b[A\n",
      " 69%|██████▉   | 69269/100000 [00:22<00:07, 3972.58it/s]\u001b[A\n",
      " 70%|██████▉   | 69680/100000 [00:22<00:07, 3828.50it/s]\u001b[A\n",
      " 70%|███████   | 70074/100000 [00:22<00:07, 3793.27it/s]\u001b[A\n",
      " 71%|███████   | 70527/100000 [00:22<00:07, 3987.45it/s]\u001b[A\n",
      " 71%|███████   | 70934/100000 [00:22<00:07, 3839.70it/s]\u001b[A\n",
      " 71%|███████▏  | 71325/100000 [00:23<00:07, 3678.94it/s]\u001b[A\n",
      " 72%|███████▏  | 71699/100000 [00:23<00:07, 3628.12it/s]\u001b[A\n",
      " 72%|███████▏  | 72220/100000 [00:23<00:06, 3991.05it/s]\u001b[A\n",
      " 73%|███████▎  | 72717/100000 [00:23<00:06, 4241.05it/s]\u001b[A\n",
      " 73%|███████▎  | 73206/100000 [00:23<00:06, 4415.40it/s]\u001b[A\n",
      " 74%|███████▎  | 73715/100000 [00:23<00:05, 4594.00it/s]\u001b[A\n",
      " 74%|███████▍  | 74205/100000 [00:23<00:05, 4680.84it/s]\u001b[A\n",
      " 75%|███████▍  | 74681/100000 [00:23<00:05, 4701.25it/s]\u001b[A\n",
      " 75%|███████▌  | 75190/100000 [00:23<00:05, 4809.47it/s]\u001b[A\n",
      " 76%|███████▌  | 75676/100000 [00:24<00:05, 4431.62it/s]\u001b[A\n",
      " 76%|███████▌  | 76216/100000 [00:24<00:05, 4682.40it/s]\u001b[A\n",
      " 77%|███████▋  | 76753/100000 [00:24<00:04, 4869.33it/s]\u001b[A\n",
      " 77%|███████▋  | 77308/100000 [00:24<00:04, 5054.39it/s]\u001b[A\n",
      " 78%|███████▊  | 77833/100000 [00:24<00:04, 5108.35it/s]\u001b[A\n",
      " 78%|███████▊  | 78350/100000 [00:24<00:04, 4683.05it/s]\u001b[A\n",
      " 79%|███████▉  | 78830/100000 [00:24<00:04, 4555.31it/s]\u001b[A\n",
      " 79%|███████▉  | 79294/100000 [00:24<00:04, 4439.46it/s]\u001b[A\n",
      " 80%|███████▉  | 79745/100000 [00:24<00:04, 4226.85it/s]\u001b[A\n",
      " 80%|████████  | 80175/100000 [00:25<00:04, 4131.40it/s]\u001b[A\n",
      " 81%|████████  | 80594/100000 [00:25<00:04, 3988.81it/s]\u001b[A\n",
      " 81%|████████  | 81006/100000 [00:25<00:04, 4025.40it/s]\u001b[A\n",
      " 81%|████████▏ | 81412/100000 [00:25<00:04, 4020.73it/s]\u001b[A\n",
      " 82%|████████▏ | 81817/100000 [00:25<00:04, 3841.75it/s]\u001b[A\n",
      " 82%|████████▏ | 82205/100000 [00:25<00:04, 3671.07it/s]\u001b[A\n",
      " 83%|████████▎ | 82577/100000 [00:25<00:04, 3647.91it/s]\u001b[A\n",
      " 83%|████████▎ | 83114/100000 [00:25<00:04, 4035.10it/s]\u001b[A\n",
      " 84%|████████▎ | 83567/100000 [00:25<00:03, 4171.51it/s]\u001b[A\n",
      " 84%|████████▍ | 84065/100000 [00:25<00:03, 4384.97it/s]\u001b[A\n",
      " 85%|████████▍ | 84518/100000 [00:26<00:03, 4425.74it/s]\u001b[A\n",
      " 85%|████████▌ | 85013/100000 [00:26<00:03, 4568.30it/s]\u001b[A\n",
      " 86%|████████▌ | 85574/100000 [00:26<00:02, 4836.01it/s]\u001b[A\n",
      " 86%|████████▌ | 86067/100000 [00:26<00:02, 4834.99it/s]\u001b[A\n",
      " 87%|████████▋ | 86580/100000 [00:26<00:02, 4917.69it/s]\u001b[A\n",
      " 87%|████████▋ | 87077/100000 [00:26<00:02, 4654.03it/s]\u001b[A\n",
      " 88%|████████▊ | 87549/100000 [00:26<00:02, 4611.69it/s]\u001b[A\n",
      " 88%|████████▊ | 88015/100000 [00:26<00:02, 4563.63it/s]\u001b[A\n",
      " 88%|████████▊ | 88475/100000 [00:26<00:02, 4391.88it/s]\u001b[A\n",
      " 89%|████████▉ | 88919/100000 [00:27<00:02, 4403.88it/s]\u001b[A\n",
      " 89%|████████▉ | 89373/100000 [00:27<00:02, 4441.62it/s]\u001b[A\n",
      " 90%|████████▉ | 89854/100000 [00:27<00:02, 4543.54it/s]\u001b[A\n",
      " 90%|█████████ | 90311/100000 [00:27<00:02, 4422.17it/s]\u001b[A\n",
      " 91%|█████████ | 90756/100000 [00:27<00:02, 4320.39it/s]\u001b[A\n",
      " 91%|█████████ | 91190/100000 [00:27<00:02, 4173.73it/s]\u001b[A\n",
      " 92%|█████████▏| 91659/100000 [00:27<00:01, 4315.09it/s]\u001b[A\n",
      " 92%|█████████▏| 92135/100000 [00:27<00:01, 4437.77it/s]\u001b[A\n",
      " 93%|█████████▎| 92637/100000 [00:27<00:01, 4595.28it/s]\u001b[A\n",
      " 93%|█████████▎| 93139/100000 [00:27<00:01, 4714.78it/s]\u001b[A\n",
      " 94%|█████████▎| 93723/100000 [00:28<00:01, 5002.49it/s]\u001b[A\n",
      " 94%|█████████▍| 94262/100000 [00:28<00:01, 5110.63it/s]\u001b[A\n",
      " 95%|█████████▍| 94804/100000 [00:28<00:00, 5197.36it/s]\u001b[A\n",
      " 95%|█████████▌| 95328/100000 [00:28<00:00, 4979.02it/s]\u001b[A\n",
      " 96%|█████████▌| 95831/100000 [00:28<00:00, 4917.96it/s]\u001b[A\n",
      " 96%|█████████▋| 96327/100000 [00:28<00:00, 4866.95it/s]\u001b[A\n",
      " 97%|█████████▋| 96817/100000 [00:28<00:00, 4723.80it/s]\u001b[A\n",
      " 97%|█████████▋| 97293/100000 [00:28<00:00, 4569.87it/s]\u001b[A\n",
      " 98%|█████████▊| 97754/100000 [00:28<00:00, 4526.33it/s]\u001b[A\n",
      " 98%|█████████▊| 98209/100000 [00:29<00:00, 4347.14it/s]\u001b[A\n",
      " 99%|█████████▊| 98672/100000 [00:29<00:00, 4427.23it/s]\u001b[A\n",
      " 99%|█████████▉| 99118/100000 [00:29<00:00, 4285.45it/s]\u001b[A\n",
      "100%|█████████▉| 99560/100000 [00:29<00:00, 4322.75it/s]\u001b[A\n",
      "100%|█████████▉| 99995/100000 [00:29<00:00, 4277.41it/s]\u001b[A\n",
      "100%|██████████| 100000/100000 [00:29<00:00, 3394.91it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "df_train['token_similarity'] = df_train[['tokens1','tokens2']].progress_apply(lambda row: token_similarity(row),axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_log_loss(true,predicted,eps=1e-15):\n",
    "    if len(true) != len(predicted):\n",
    "        print(\"True and predicted values need to be of same shape.\")\n",
    "        return 0\n",
    "\n",
    "    true = true.astype(\"float64\")\n",
    "    predicted = predicted.astype(\"float64\")\n",
    "    \n",
    "    predicted = np.minimum(np.maximum(predicted,eps),1-eps)\n",
    "    intermediate = true * np.log(predicted) + (1-true) * np.log(1-predicted)\n",
    "    score = -np.mean(intermediate)\n",
    "    return score\n",
    "\n",
    "def getModelStats(model, testing_data, testing_target):\n",
    "    test_x = torch.from_numpy(testing_data).float()\n",
    "\n",
    "    results = model(test_x).detach().numpy()\n",
    "    results2 = np.round(results)\n",
    "    error = np.sum(np.abs(results2-testing_target))/len(testing_target)\n",
    "\n",
    "    log_loss = calc_log_loss(testing_target,results)\n",
    "\n",
    "    print(\"Error: \"+str(error))\n",
    "    print(\"Accuracy: \"+str(1-error))\n",
    "    print(\"Log-loss: \"+str(log_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 1\n",
    "## Fully Connected - 1 Hidden Layer - MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_columns = ['firstWordSame','lastWordSame','embedding_similarity','embedding_similarity_tfidf','word_ratio','char_ratio','math_similarity','token_similarity']\n",
    "\n",
    "df_tr = df_train[:80000]\n",
    "df_te = df_train[80000:]\n",
    "\n",
    "training_target = df_tr['is_duplicate'].values.reshape((len(df_tr), 1))\n",
    "training_data = df_tr[pred_columns].values\n",
    "\n",
    "testing_target = df_te['is_duplicate'].values.reshape((len(df_te), 1))\n",
    "testing_data = df_te[pred_columns].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.26189756393432617\n",
      "500 0.18413563072681427\n",
      "1000 0.18403802812099457\n",
      "1500 0.18010516464710236\n",
      "2000 0.17978347837924957\n",
      "2500 0.17948423326015472\n",
      "3000 0.17828193306922913\n",
      "3500 0.17627638578414917\n",
      "4000 0.175069659948349\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(training_data).float()\n",
    "y = torch.from_numpy(training_target).float()\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(8, 8),\n",
    "          torch.nn.Softmax(dim=1),\n",
    "          torch.nn.Linear(8, 1),\n",
    "          torch.nn.Sigmoid()\n",
    "        )\n",
    "#loss_fn = torch.nn.BCELoss()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 5e-2\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(4001):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if (t) % 500 == 0:\n",
    "        print((t), loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the Tensors it will update (which are the learnable weights\n",
    "    # of the model)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.273\n",
      "Accuracy: 0.727\n",
      "Log-loss: 0.5103850184193128\n"
     ]
    }
   ],
   "source": [
    "getModelStats(model, testing_data, testing_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 2\n",
    "## Vectors, convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildVectorModel(column, df_tr, df_te, training_target, testing_target):\n",
    "    training_data_pre = df_tr[[column]].values\n",
    "    training_data = []\n",
    "    for vector in training_data_pre:\n",
    "        training_data.append(vector[0])\n",
    "    training_data = np.array(training_data)\n",
    "\n",
    "    testing_data_pre = df_te[[column]].values\n",
    "    testing_data = []\n",
    "    for vector in testing_data_pre:\n",
    "        testing_data.append(vector[0])\n",
    "    testing_data = np.array(testing_data)\n",
    "    \n",
    "    x = torch.from_numpy(training_data).float()\n",
    "    y = torch.from_numpy(training_target).float()\n",
    "\n",
    "    v_model = torch.nn.Sequential(\n",
    "              torch.nn.Linear(300, 100),\n",
    "              torch.nn.ReLU(),\n",
    "              torch.nn.Linear(100, 20),\n",
    "              torch.nn.ReLU(),\n",
    "              torch.nn.Linear(20, 1),\n",
    "              torch.nn.Sigmoid()\n",
    "            )\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    learning_rate = 5e-2\n",
    "    optimizer = torch.optim.Adam(v_model.parameters(), lr=learning_rate)\n",
    "    for t in range(1001):\n",
    "        y_pred = v_model(x)\n",
    "\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        if (t) % 100 == 0:\n",
    "            print((t), loss.item())\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "    getModelStats(v_model, testing_data, testing_target)\n",
    "    \n",
    "    return v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.26593101024627686\n",
      "100 0.174383282661438\n",
      "200 0.160793274641037\n",
      "300 0.15550464391708374\n",
      "400 0.1549474149942398\n",
      "500 0.1526605784893036\n",
      "600 0.14689694344997406\n",
      "700 0.14630483090877533\n",
      "800 0.1447707861661911\n",
      "900 0.1471860557794571\n",
      "1000 0.14673252403736115\n",
      "Error: 0.2597\n",
      "Accuracy: 0.7403\n",
      "Log-loss: 0.5596444850941115\n"
     ]
    }
   ],
   "source": [
    "model2 = buildVectorModel('vector_combo1', df_tr, df_te, training_target, testing_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2406657189130783\n",
      "100 0.23376934230327606\n",
      "200 0.2337692379951477\n",
      "300 0.2337692379951477\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-4cc98d7bcc90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildVectorModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vector_combo2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-110-87e9f2c89640>\u001b[0m in \u001b[0;36mbuildVectorModel\u001b[0;34m(column, df_tr, df_te, training_target, testing_target)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model3 = buildVectorModel('vector_combo2', df_tr, df_te, training_target, testing_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2991880178451538\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-d9a359236764>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildVectorModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vector_combo3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-110-87e9f2c89640>\u001b[0m in \u001b[0;36mbuildVectorModel\u001b[0;34m(column, df_tr, df_te, training_target, testing_target)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model4 = buildVectorModel('vector_combo3', df_tr, df_te, training_target, testing_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = buildVectorModel('vector_combo4', df_tr, df_te, training_target, testing_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = model(test_x).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
