{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unidecode\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import operator\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nltk import ngrams\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.corpus import stopwords\n",
    "from pyjarowinkler import distance\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_types = ['PERSON','NORP','FAC','ORG','GPE','LOC','PRODUCT','DATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_model = \"en_core_web_lg\"\n",
    "\n",
    "tokenizer = re.compile(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveOBJ(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def loadOBJ(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing spaCy \"en_core_web_lg\"...\n",
      "Done!\n",
      "Time elapsed: 14s\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"Importing spaCy \\\"\"+spacy_model+\"\\\"...\")\n",
    "nlp = spacy.load(spacy_model)\n",
    "print(\"Done!\")\n",
    "print(\"Time elapsed: \"+str(round(time.time()-start))+\"s\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "df_train = pd.read_csv(\"data/train_data.csv\",index_col=\"id\")\n",
    "df_train = df_train.drop(\"is_duplicate\",axis=1)\n",
    "\n",
    "df_labels = pd.read_csv(\"data/train_labels.csv\",index_col=\"id\")\n",
    "\n",
    "df_train = df_train.join(df_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question1  \\\n",
       "id                                                      \n",
       "0   What is the step by step guide to invest in sh...   \n",
       "1   What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   How can I increase the speed of my internet co...   \n",
       "3   Why am I mentally very lonely? How can I solve...   \n",
       "4   Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                            question2  is_duplicate  \n",
       "id                                                                   \n",
       "0   What is the step by step guide to invest in sh...             0  \n",
       "1   What would happen if the Indian government sto...             0  \n",
       "2   How can Internet speed be increased by hacking...             0  \n",
       "3   Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4             Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add graph info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_graphs = pd.read_csv(\"gen_data/train_graph.csv\",index_col=\"id\")\n",
    "\n",
    "df_train = df_train.join(df_training_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What would a Trump presidency mean for current...</td>\n",
       "      <td>How will a Trump presidency affect the student...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Why do rockets look white?</td>\n",
       "      <td>Why are rockets and boosters painted white?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What's causing someone to be jealous?</td>\n",
       "      <td>What can I do to avoid being jealous of someone?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>How much is 30 kV in HP?</td>\n",
       "      <td>Where can I find a conversion chart for CC to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>What is the best travel website in spain?</td>\n",
       "      <td>What is the best travel website?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 question1  \\\n",
       "test_id                                                      \n",
       "15       What would a Trump presidency mean for current...   \n",
       "20                              Why do rockets look white?   \n",
       "21                   What's causing someone to be jealous?   \n",
       "23                                How much is 30 kV in HP?   \n",
       "34               What is the best travel website in spain?   \n",
       "\n",
       "                                                 question2  \n",
       "test_id                                                     \n",
       "15       How will a Trump presidency affect the student...  \n",
       "20             Why are rockets and boosters painted white?  \n",
       "21        What can I do to avoid being jealous of someone?  \n",
       "23       Where can I find a conversion chart for CC to ...  \n",
       "34                        What is the best travel website?  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing data\n",
    "df_train = pd.read_csv(\"data/test_data.csv\",index_col=\"test_id\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing_graphs = pd.read_csv(\"gen_data/test_graph_all.csv\",index_col=\"test_id\")\n",
    "\n",
    "df_train = df_train.join(df_testing_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_counts = {}\n",
    "\n",
    "# print(\"Starting with \"+str(len(df_train))+ \" rows..\")\n",
    "\n",
    "# for index, row in df_train.iterrows():\n",
    "#     docs = [row['question1'], row['question2']]\n",
    "#     for doc in docs:\n",
    "#         tokens = tokenizer.findall(str(doc).lower())\n",
    "#         for token in tokens:\n",
    "#             if token in token_counts:\n",
    "#                 token_counts[token] += 1\n",
    "#             else:\n",
    "#                 token_counts[token] = 1\n",
    "                \n",
    "#     if (index+1) % 10000 == 0:\n",
    "#         print(str(index+1)+\" rows done..\")\n",
    "\n",
    "# print()\n",
    "# print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_docs = 2*len(df_train)\n",
    "\n",
    "# token_idf = {}\n",
    "# for token in token_counts:\n",
    "#     token_idf[token] = np.log(n_docs/(token_counts[token]))\n",
    "    \n",
    "# saveOBJ(token_idf,\"data/token_IDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_idf = loadOBJ(\"data/token_IDF\")\n",
    "sorted_idf = sorted(token_idf.items(), key=operator.itemgetter(1),reverse=True)\n",
    "max_idf = sorted_idf[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "#df_train = df_train[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* first word same (also fuzzy)\n",
    "* last word same (also fuzzy)\n",
    "* length ratio\n",
    "* n_words ratio\n",
    "* context embedding similarity (also with TF-IDF)\n",
    "* token one-hot encoding similarity (also with TF-IDF)\n",
    "* both contain (or don't contain) math\n",
    "* graph network features\n",
    "\n",
    "Todo:\n",
    "* NER->both contain (or don't contain) same entity\n",
    "* Jaro distance\n",
    "* Lehvenstein distance\n",
    "* bi,tri,4-grams\n",
    "* Fuzzy one-hot (&TF-IDF) token matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absmaxND(a, axis=None):\n",
    "    amax = a.max(axis)\n",
    "    amin = a.min(axis)\n",
    "    return np.where(-amin > amax, amin, amax)\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = tokenizer.findall(str(text).lower())\n",
    "    return tokens\n",
    "\n",
    "def entities(question):\n",
    "    try:\n",
    "        doc = nlp(question)\n",
    "        ents = []\n",
    "        for ent in doc.ents:\n",
    "            ents.append([ent.text,ent.label_])\n",
    "    except:\n",
    "        ents = []\n",
    "    return ents\n",
    "\n",
    "def entity_score(row):\n",
    "    question1 = str(row[\"question1\"]).lower()\n",
    "    question2 = str(row[\"question2\"]).lower()\n",
    "    \n",
    "    entities1 = []\n",
    "    for ent in row[\"entities1\"]:\n",
    "        if ent[1] in ent_types:\n",
    "            entities1.append(ent[0].lower())\n",
    "            \n",
    "    entities2 = []\n",
    "    for ent in row[\"entities2\"]:\n",
    "        if ent[1] in ent_types:\n",
    "            entities2.append(ent[0].lower())\n",
    "    \n",
    "    if len(entities1) == 0 and len(entities2) == 0:\n",
    "        return 1\n",
    "    \n",
    "    ent1score = 0\n",
    "    ent1maxscore = len(entities1)\n",
    "    for ent1 in entities1:\n",
    "        subscore = 0\n",
    "        for ent2 in entities2:\n",
    "            if ent1 == ent2:\n",
    "                subscore += 1\n",
    "                continue\n",
    "            if edit_distance(ent1, ent2) < len(ent1)/4:\n",
    "                subscore += 1\n",
    "                continue\n",
    "            if ent1 in ent2:\n",
    "                subscore += 1\n",
    "                continue\n",
    "            if ent2 in ent1:\n",
    "                subscore += 1\n",
    "                continue\n",
    "        if ent1 in question2:\n",
    "            subscore += 1\n",
    "        ent1score += np.minimum(subscore,1)\n",
    "    ent1score = np.minimum(ent1score,ent1maxscore)\n",
    "    \n",
    "    ent2score = 0\n",
    "    ent2maxscore = len(entities2)\n",
    "    for ent2 in entities2:\n",
    "        subscore = 0\n",
    "        for ent1 in entities1:\n",
    "            if ent1 == ent2:\n",
    "                subscore += 1\n",
    "                continue\n",
    "            if edit_distance(ent1, ent2) < len(ent2)/4:\n",
    "                subscore += 1\n",
    "                continue\n",
    "            if ent1 in ent2:\n",
    "                subscore += 1\n",
    "                continue\n",
    "            if ent2 in ent1:\n",
    "                subscore += 1\n",
    "                continue\n",
    "        if ent2 in question1:\n",
    "            subscore += 1\n",
    "        ent2score += np.minimum(subscore,1)\n",
    "    ent2score = np.minimum(ent2score,ent2maxscore)\n",
    "    \n",
    "    score = (ent1score+ent2score)/(ent1maxscore+ent2maxscore)\n",
    "    return score\n",
    "\n",
    "def check_word(row,word=\"why\"):\n",
    "    if len(row['tokens1']) == 0 or len(row['tokens2']) == 0:\n",
    "        return 1\n",
    "    token_sets = [row['tokens1'],row['tokens2']]\n",
    "    founds = []\n",
    "    for token_set in token_sets:\n",
    "        if word in token_set:\n",
    "            founds.append(True)\n",
    "        else:\n",
    "            founds.append(False)\n",
    "    if founds[0] == founds[1]:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def word_length_ratio(row):\n",
    "    if len(row['tokens1']) == 0 or len(row['tokens2']) == 0:\n",
    "        return 0\n",
    "    token_sets = [row['tokens1'],row['tokens2']]\n",
    "    avg_lengths = []\n",
    "    for token_set in token_sets:\n",
    "        length = 0\n",
    "        for token in token_set:\n",
    "            length += len(token)\n",
    "        avg_lengths.append(length/len(token_set))\n",
    "    ratio = avg_lengths[0]/avg_lengths[1]\n",
    "    ratio = np.minimum(ratio,1/ratio)\n",
    "    return ratio\n",
    "        \n",
    "def stop_word_ratio(row):\n",
    "    if len(row['tokens1']) == 0 or len(row['tokens2']) == 0:\n",
    "        return 0\n",
    "    token_sets = [row['tokens1'],row['tokens2']]\n",
    "    stop_word_parts = []\n",
    "    for token_set in token_sets:\n",
    "        stops = 0\n",
    "        for token in token_set:\n",
    "            if token in stopWords:\n",
    "                stops += 1\n",
    "        stop_word_parts.append(stops/len(token_set))\n",
    "    try:\n",
    "        ratio = stop_word_parts[0]/stop_word_parts[1]\n",
    "        ratio = np.minimum(ratio,1/ratio)\n",
    "    except:\n",
    "        ratio = 0\n",
    "    return ratio\n",
    "\n",
    "def UC_ratio(row):\n",
    "    if len(str(row['question1'])) == 0 or len(str(row['question2'])) == 0:\n",
    "        return 0\n",
    "    questions = [row['question1'],row['question2']]\n",
    "    UCS = []\n",
    "    for question in questions:\n",
    "        UCsum = sum(1 for c in str(question) if c.isupper())\n",
    "        UCS.append(UCsum)\n",
    "    try:\n",
    "        ratio = UCS[0]/UCS[1]\n",
    "        ratio = np.minimum(ratio,1/ratio)\n",
    "    except:\n",
    "        ratio = 0\n",
    "    return ratio\n",
    "    \n",
    "def tok_ngrams(row,n=2):\n",
    "    if len(row['tokens1']) == 0 or len(row['tokens2']) == 0:\n",
    "        return 0\n",
    "    token_sets = [row['tokens1'],row['tokens2']]\n",
    "    gram_sets = []\n",
    "    try:\n",
    "        for token_set in token_sets:\n",
    "            gram_set = []\n",
    "            for gram in ngrams(token_set, n):\n",
    "                gram_set.append(gram)\n",
    "            gram_sets.append(gram_set)\n",
    "        L1 = len(gram_sets[0]) + len(gram_sets[1])\n",
    "        L2 = len(list(set(gram_sets[0] + gram_sets[1])))\n",
    "        if L1 > 0 and L2 > 0:\n",
    "            score = L1/L2 - 1\n",
    "        else:\n",
    "            score = 0\n",
    "    except:\n",
    "        score = 0\n",
    "    return score\n",
    "    \n",
    "def edit_distance_score(row):\n",
    "    try:\n",
    "        distance = edit_distance(row[\"question1\"].lower(), row[\"question2\"].lower())\n",
    "        length = np.maximum(len(row[\"question1\"]),len(row[\"question2\"]))\n",
    "        if distance == 0 and len(row[\"question1\"]) > 0 and len(row[\"question2\"]) > 0:\n",
    "            score = 2\n",
    "        else:\n",
    "            score = (length/distance)/50\n",
    "    except:\n",
    "        score = 0\n",
    "    return score\n",
    "\n",
    "def jaro_similarity(row):\n",
    "    try:\n",
    "        score = distance.get_jaro_distance(row[\"question1\"].lower(), row[\"question2\"].lower(), winkler=False)\n",
    "    except:\n",
    "        score = 0\n",
    "    return score\n",
    "\n",
    "def firstWordSame(row):\n",
    "    if len(row['tokens1']) == 0 or len(row['tokens2']) == 0:\n",
    "        return 0\n",
    "    if row['tokens1'][0] == row['tokens2'][0]:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def lastWordSame(row):\n",
    "    if len(row['tokens1']) == 0 or len(row['tokens2']) == 0:\n",
    "        return 0\n",
    "    if row['tokens1'][-1] == row['tokens2'][-1]:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def word_ratio(row):\n",
    "    if len(row['tokens1']) == 0 or len(row['tokens2']) == 0:\n",
    "        return 0\n",
    "    ratio = len(row['tokens1'])/len(row['tokens2'])\n",
    "    if ratio > 1:\n",
    "        return 1/ratio\n",
    "    return ratio\n",
    "\n",
    "def char_ratio(row):\n",
    "    if len(row['tokens1']) == 0 or len(row['tokens2']) == 0:\n",
    "        return 0\n",
    "    ratio = len(\"\".join(row['tokens1']))/len(\"\".join(row['tokens2']))\n",
    "    if ratio > 1:\n",
    "        return 1/ratio\n",
    "    return ratio\n",
    "\n",
    "def math_similarity(row):\n",
    "    hit1 = 0\n",
    "    hit2 = 0\n",
    "    if \"[math]\" in str(row[\"question1\"]):\n",
    "        hit1 = 1\n",
    "    if \"[math]\" in str(row[\"question2\"]):\n",
    "        hit2 = 1\n",
    "    if hit1 == hit2:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def getVector(tokens,TFIDF=False,mode=\"AVG\"):\n",
    "    if len(tokens) == 0:\n",
    "        return 0\n",
    "    vectors = []\n",
    "    totalWeight = 0\n",
    "    for token in tokens:\n",
    "        token_id = nlp.vocab.strings[token]\n",
    "        try:\n",
    "            weight = 1\n",
    "            if TFIDF:\n",
    "                if token in token_idf:\n",
    "                    weight = token_idf[token]\n",
    "                else:\n",
    "                    weight = max_idf\n",
    "            vectors.append(nlp.vocab.vectors[token_id]*weight)\n",
    "            totalWeight += weight\n",
    "        except:\n",
    "            continue\n",
    "    if len(vectors) == 0:\n",
    "        return 0\n",
    "    vectors = np.array(vectors)\n",
    "    if mode == \"AVG\":\n",
    "        vector = np.sum(vectors,axis=0)/totalWeight\n",
    "    elif mode == \"MAX\":\n",
    "        vector = absmaxND(vectors,axis=0)\n",
    "    if np.linalg.norm(vector) == 0:\n",
    "        return 0\n",
    "    return vector\n",
    "\n",
    "def cosine_similarity(vectors):\n",
    "    v1 = vectors[0]\n",
    "    v2 = vectors[1]\n",
    "    if np.linalg.norm(v1) == 0:\n",
    "        return 0\n",
    "    elif np.linalg.norm(v2) == 0:\n",
    "        return 0\n",
    "    similarity = np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def multiply_vectors(vectors):\n",
    "    v1 = vectors[0]\n",
    "    v2 = vectors[1]\n",
    "    if np.linalg.norm(v1) == 0:\n",
    "        return np.zeros(300)\n",
    "    elif np.linalg.norm(v2) == 0:\n",
    "        return np.zeros(300)\n",
    "    product = v1*v2\n",
    "    return product\n",
    "\n",
    "def add_vectors(vectors):\n",
    "    v1 = vectors[0]\n",
    "    v2 = vectors[1]\n",
    "    if np.linalg.norm(v1) == 0:\n",
    "        return np.zeros(300)\n",
    "    elif np.linalg.norm(v2) == 0:\n",
    "        return np.zeros(300)\n",
    "    added = np.absolute(v1+v2)\n",
    "    return added\n",
    "\n",
    "def token_similarity(row, TFIDF=False):\n",
    "    if len(row['tokens1']) == 0 or len(row['tokens2']) == 0:\n",
    "        return 0\n",
    "    \n",
    "    #tokens of each question, including repeated words\n",
    "    token_sets = [row['tokens1'],row['tokens2']]\n",
    "    \n",
    "    #total unique tokens across both questions\n",
    "    total_tokens = list(set(row['tokens1']+row['tokens2']))\n",
    "    \n",
    "    vectors = []\n",
    "    for token_set in token_sets:\n",
    "        token_vector = np.zeros(len(total_tokens))\n",
    "        for token in token_set:\n",
    "            weight = 1\n",
    "            if TFIDF:\n",
    "                if token in token_idf:\n",
    "                    weight = token_idf[token]\n",
    "                else:\n",
    "                    weight = max_idf\n",
    "            token_vector[total_tokens.index(token)] += weight\n",
    "        if np.linalg.norm(token_vector) == 0:\n",
    "            return 0\n",
    "        token_vector = token_vector/np.linalg.norm(token_vector)\n",
    "        vectors.append(token_vector)\n",
    "\n",
    "    #only dot product is sufficient because already normalized\n",
    "    similarity = np.dot(vectors[0],vectors[1])\n",
    "    return similarity\n",
    "\n",
    "def word_match_fuzzy(row, first=True):\n",
    "    if len(row['tokens1']) == 0 or len(row['tokens2']) == 0:\n",
    "        return 0\n",
    "    vectors = []\n",
    "    \n",
    "    if first:\n",
    "        tokens = [row['tokens1'][0], row['tokens2'][0]]\n",
    "    else:\n",
    "        tokens = [row['tokens1'][-1], row['tokens2'][-1]]\n",
    "    \n",
    "    for token in tokens:\n",
    "        token_id = nlp.vocab.strings[token]\n",
    "        try:\n",
    "            vector = nlp.vocab.vectors[token_id]\n",
    "            if np.linalg.norm(vector) == 0:\n",
    "                continue\n",
    "            vectors.append(vector)\n",
    "        except:\n",
    "            continue\n",
    "    if len(vectors) != 2:\n",
    "        return 0\n",
    "    vectors = np.array(vectors)\n",
    "    return cosine_similarity(vectors)\n",
    "\n",
    "def cross_correlation(row, TFIDF=False):\n",
    "    if len(row['tokens1']) == 0 or len(row['tokens2']) == 0:\n",
    "        return 0\n",
    "    \n",
    "    #tokens of each question, including repeated words\n",
    "    token_sets = [row['tokens1'],row['tokens2']]\n",
    "    \n",
    "    vectors = []\n",
    "    weights = []\n",
    "    for token_set in token_sets:\n",
    "        subset = []\n",
    "        subweights = []\n",
    "        for token in token_set:\n",
    "            token_id = nlp.vocab.strings[token]\n",
    "            try:\n",
    "                weight = 1\n",
    "                if TFIDF:\n",
    "                    if token in token_idf:\n",
    "                        weight = token_idf[token]\n",
    "                    else:\n",
    "                        weight = max_idf\n",
    "                subset.append(nlp.vocab.vectors[token_id])\n",
    "                subweights.append(weight)\n",
    "            except:\n",
    "                continue\n",
    "        if len(subset) == 0:\n",
    "            return 0\n",
    "        vectors.append(subset)\n",
    "        weights.append(subweights)\n",
    "    \n",
    "    scores = []\n",
    "    weight_products = []\n",
    "    \n",
    "    for i, v1 in enumerate(vectors[0]):\n",
    "        for j, v2 in enumerate(vectors[1]):\n",
    "            combi_weight = np.sqrt(weights[0][i]*weights[1][j])\n",
    "            scores.append(cosine_similarity([v1,v2])*combi_weight)\n",
    "            weight_products.append(combi_weight)\n",
    "            \n",
    "    correlation_score = np.sum(scores)/np.sum(weight_products)\n",
    "    return correlation_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 323164/323164 [00:03<00:00, 106869.55it/s]\n",
      "100%|██████████| 323164/323164 [00:03<00:00, 96320.98it/s] \n"
     ]
    }
   ],
   "source": [
    "df_train['tokens1'] = df_train['question1'].progress_apply(lambda x: tokenize(x))\n",
    "df_train['tokens2'] = df_train['question2'].progress_apply(lambda x: tokenize(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81126/81126 [00:08<00:00, 9139.20it/s] \n",
      "100%|██████████| 81126/81126 [00:09<00:00, 8967.86it/s] \n"
     ]
    }
   ],
   "source": [
    "df_train['vector1'] = df_train['tokens1'].progress_apply(lambda x: getVector(x))\n",
    "df_train['vector2'] = df_train['tokens2'].progress_apply(lambda x: getVector(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81126/81126 [00:10<00:00, 7945.47it/s]\n",
      "100%|██████████| 81126/81126 [00:09<00:00, 8522.56it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['vector_tfidf1'] = df_train['tokens1'].progress_apply(lambda x: getVector(x,TFIDF=True))\n",
    "df_train['vector_tfidf2'] = df_train['tokens2'].progress_apply(lambda x: getVector(x,TFIDF=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81126/81126 [00:09<00:00, 8431.31it/s]\n",
      "100%|██████████| 81126/81126 [00:10<00:00, 7624.53it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['embedding_similarity'] = df_train[['vector1','vector2']].progress_apply(lambda row: cosine_similarity(row),axis=1)\n",
    "df_train['embedding_similarity_tfidf'] = df_train[['vector_tfidf1','vector_tfidf2']].progress_apply(lambda row: cosine_similarity(row),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81126/81126 [00:15<00:00, 5231.78it/s]\n",
      "100%|██████████| 81126/81126 [00:13<00:00, 5862.80it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['vector_combo1'] = df_train[['vector1','vector2']].progress_apply(lambda row: multiply_vectors(row),axis=1)\n",
    "df_train['vector_combo2'] = df_train[['vector_tfidf1','vector_tfidf2']].progress_apply(lambda row: multiply_vectors(row),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81126/81126 [00:04<00:00, 18630.49it/s]\n",
      "100%|██████████| 81126/81126 [00:04<00:00, 19394.83it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['firstWordSame'] = df_train[['tokens1','tokens2']].progress_apply(lambda row: firstWordSame(row),axis=1)\n",
    "df_train['lastWordSame'] = df_train[['tokens1','tokens2']].progress_apply(lambda row: lastWordSame(row),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81126/81126 [00:11<00:00, 7224.03it/s]\n",
      "100%|██████████| 81126/81126 [00:13<00:00, 5905.15it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['firstWordSame_fuzzy'] = df_train[['tokens1','tokens2']].progress_apply(lambda row: word_match_fuzzy(row),axis=1)\n",
    "df_train['lastWordSame_fuzzy'] = df_train[['tokens1','tokens2']].progress_apply(lambda row: word_match_fuzzy(row, first=False),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81126/81126 [00:04<00:00, 17145.48it/s]\n",
      "100%|██████████| 81126/81126 [00:04<00:00, 17760.74it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['word_ratio'] = df_train[['tokens1','tokens2']].progress_apply(lambda row: word_ratio(row),axis=1)\n",
    "df_train['char_ratio'] = df_train[['tokens1','tokens2']].progress_apply(lambda row: char_ratio(row),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81126/81126 [00:02<00:00, 37128.67it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['math_similarity'] = df_train[['question1','question2']].progress_apply(lambda row: math_similarity(row),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81126/81126 [00:16<00:00, 5016.58it/s]\n",
      "100%|██████████| 81126/81126 [00:16<00:00, 4884.63it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['token_similarity'] = df_train[['tokens1','tokens2']].progress_apply(lambda row: token_similarity(row),axis=1)\n",
    "df_train['token_similarity_idf'] = df_train[['tokens1','tokens2']].progress_apply(lambda row: token_similarity(row, TFIDF=True),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81126/81126 [09:02<00:00, 113.29it/s]\n",
      "100%|██████████| 81126/81126 [12:35<00:00, 107.45it/s] \n"
     ]
    }
   ],
   "source": [
    "df_train['cross_correlation'] = df_train[['tokens1','tokens2']].progress_apply(lambda row: cross_correlation(row),axis=1)\n",
    "df_train['cross_correlation_idf'] = df_train[['tokens1','tokens2']].progress_apply(lambda row: cross_correlation(row, TFIDF=True),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81126/81126 [00:06<00:00, 11928.40it/s]\n",
      "100%|██████████| 81126/81126 [00:06<00:00, 13142.66it/s]\n",
      "100%|██████████| 81126/81126 [00:06<00:00, 12957.94it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['2grams'] =  df_train[['tokens1','tokens2']].progress_apply(lambda row: tok_ngrams(row,n=2),axis=1)\n",
    "df_train['3grams'] =  df_train[['tokens1','tokens2']].progress_apply(lambda row: tok_ngrams(row,n=3),axis=1)\n",
    "df_train['4grams'] =  df_train[['tokens1','tokens2']].progress_apply(lambda row: tok_ngrams(row,n=4),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81126/81126 [05:57<00:00, 226.90it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['edit_score'] = df_train[['question1','question2']].progress_apply(lambda row: edit_distance_score(row),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81126/81126 [00:20<00:00, 3913.01it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['jaro_score'] = df_train[['question1','question2']].progress_apply(lambda row: jaro_similarity(row),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81126/81126 [00:06<00:00, 12910.84it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['wl_ratio'] =  df_train[['tokens1','tokens2']].progress_apply(lambda row: word_length_ratio(row),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81126/81126 [00:05<00:00, 14080.82it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['stop_ratio'] =  df_train[['tokens1','tokens2']].progress_apply(lambda row: stop_word_ratio(row),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81126/81126 [00:05<00:00, 15447.59it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['UC_ratio'] =  df_train[['question1','question2']].progress_apply(lambda row: UC_ratio(row),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81126/81126 [00:05<00:00, 15844.94it/s]\n",
      "100%|██████████| 81126/81126 [00:04<00:00, 16883.29it/s]\n",
      "100%|██████████| 81126/81126 [00:05<00:00, 14621.69it/s]\n",
      "100%|██████████| 81126/81126 [00:04<00:00, 17450.90it/s]\n",
      "100%|██████████| 81126/81126 [00:04<00:00, 17560.26it/s]\n",
      "100%|██████████| 81126/81126 [00:05<00:00, 14096.22it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['why'] =  df_train[['tokens1','tokens2']].progress_apply(lambda row: check_word(row,word=\"why\"),axis=1)\n",
    "df_train['what'] =  df_train[['tokens1','tokens2']].progress_apply(lambda row: check_word(row,word=\"what\"),axis=1)\n",
    "df_train['when'] =  df_train[['tokens1','tokens2']].progress_apply(lambda row: check_word(row,word=\"when\"),axis=1)\n",
    "df_train['where'] =  df_train[['tokens1','tokens2']].progress_apply(lambda row: check_word(row,word=\"where\"),axis=1)\n",
    "df_train['how'] =  df_train[['tokens1','tokens2']].progress_apply(lambda row: check_word(row,word=\"how\"),axis=1)\n",
    "df_train['who'] =  df_train[['tokens1','tokens2']].progress_apply(lambda row: check_word(row,word=\"who\"),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81126/81126 [16:58<00:00, 79.66it/s]  \n",
      "100%|██████████| 81126/81126 [15:03<00:00, 89.81it/s] \n"
     ]
    }
   ],
   "source": [
    "df_train['entities1'] = df_train['question1'].progress_apply(lambda x: entities(x))\n",
    "df_train['entities2'] = df_train['question2'].progress_apply(lambda x: entities(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81126/81126 [00:09<00:00, 8208.34it/s]\n",
      "100%|██████████| 81126/81126 [00:09<00:00, 8657.89it/s]\n",
      "100%|██████████| 81126/81126 [00:10<00:00, 7595.47it/s]\n",
      "100%|██████████| 81126/81126 [00:10<00:00, 7419.91it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['vector_max1'] = df_train['tokens1'].progress_apply(lambda x: getVector(x,mode=\"MAX\"))\n",
    "df_train['vector_max2'] = df_train['tokens2'].progress_apply(lambda x: getVector(x,mode=\"MAX\"))\n",
    "df_train['vector_max_tfidf1'] = df_train['tokens1'].progress_apply(lambda x: getVector(x,TFIDF=True,mode=\"MAX\"))\n",
    "df_train['vector_max_tfidf2'] = df_train['tokens2'].progress_apply(lambda x: getVector(x,TFIDF=True,mode=\"MAX\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81126/81126 [00:09<00:00, 8370.73it/s] \n",
      "100%|██████████| 81126/81126 [00:10<00:00, 7989.54it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['embedding_similarity_max'] = df_train[['vector_max1','vector_max2']].progress_apply(lambda row: cosine_similarity(row),axis=1)\n",
    "df_train['embedding_similarity_max_tfidf'] = df_train[['vector_max_tfidf1','vector_max_tfidf2']].progress_apply(lambda row: cosine_similarity(row),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81126/81126 [00:23<00:00, 3429.23it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['entity_score'] = df_train[['question1','question2','entities1','entities2']].progress_apply(lambda row: entity_score(row),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train = loadOBJ(\"gen_data/df_train\")\n",
    "df_train = loadOBJ(\"gen_data/df_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>tokens1</th>\n",
       "      <th>tokens2</th>\n",
       "      <th>embedding_similarity</th>\n",
       "      <th>embedding_similarity_tfidf</th>\n",
       "      <th>vector_combo1</th>\n",
       "      <th>vector_combo2</th>\n",
       "      <th>firstWordSame</th>\n",
       "      <th>lastWordSame</th>\n",
       "      <th>...</th>\n",
       "      <th>what</th>\n",
       "      <th>when</th>\n",
       "      <th>where</th>\n",
       "      <th>how</th>\n",
       "      <th>who</th>\n",
       "      <th>embedding_similarity_max</th>\n",
       "      <th>embedding_similarity_max_tfidf</th>\n",
       "      <th>entities1</th>\n",
       "      <th>entities2</th>\n",
       "      <th>entity_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What would a Trump presidency mean for current...</td>\n",
       "      <td>How will a Trump presidency affect the student...</td>\n",
       "      <td>[what, would, a, trump, presidency, mean, for,...</td>\n",
       "      <td>[how, will, a, trump, presidency, affect, the,...</td>\n",
       "      <td>0.894801</td>\n",
       "      <td>0.826423</td>\n",
       "      <td>[0.0011587327, 0.009684816, 0.001439825, 0.000...</td>\n",
       "      <td>[0.00010098619, 0.0010915771, 0.020145686, 0.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.520753</td>\n",
       "      <td>0.397651</td>\n",
       "      <td>[[Trump, ORG]]</td>\n",
       "      <td>[[Trump, ORG], [US, GPE], [US, GPE]]</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Why do rockets look white?</td>\n",
       "      <td>Why are rockets and boosters painted white?</td>\n",
       "      <td>[why, do, rockets, look, white]</td>\n",
       "      <td>[why, are, rockets, and, boosters, painted, wh...</td>\n",
       "      <td>0.871040</td>\n",
       "      <td>0.846412</td>\n",
       "      <td>[0.06817112, -5.824842e-05, 0.106069826, 0.044...</td>\n",
       "      <td>[0.11919739, 0.0032099667, 0.10251035, 0.04612...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.743635</td>\n",
       "      <td>0.737955</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What's causing someone to be jealous?</td>\n",
       "      <td>What can I do to avoid being jealous of someone?</td>\n",
       "      <td>[what, s, causing, someone, to, be, jealous]</td>\n",
       "      <td>[what, can, i, do, to, avoid, being, jealous, ...</td>\n",
       "      <td>0.948524</td>\n",
       "      <td>0.924701</td>\n",
       "      <td>[0.010460816, 0.029607525, 0.08545068, 0.00066...</td>\n",
       "      <td>[0.06137621, -0.0012372601, 0.070046395, 0.002...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.634457</td>\n",
       "      <td>0.749039</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>How much is 30 kV in HP?</td>\n",
       "      <td>Where can I find a conversion chart for CC to ...</td>\n",
       "      <td>[how, much, is, 30, kv, in, hp]</td>\n",
       "      <td>[where, can, i, find, a, conversion, chart, fo...</td>\n",
       "      <td>0.804563</td>\n",
       "      <td>0.559170</td>\n",
       "      <td>[-0.0070741503, 0.16010112, -0.04363538, 0.002...</td>\n",
       "      <td>[0.015298527, 0.22946647, -0.00915852, 0.01152...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.411248</td>\n",
       "      <td>0.259039</td>\n",
       "      <td>[[30 kV, QUANTITY], [HP, PRODUCT]]</td>\n",
       "      <td>[[CC, ORG]]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>What is the best travel website in spain?</td>\n",
       "      <td>What is the best travel website?</td>\n",
       "      <td>[what, is, the, best, travel, website, in, spain]</td>\n",
       "      <td>[what, is, the, best, travel, website]</td>\n",
       "      <td>0.962578</td>\n",
       "      <td>0.874730</td>\n",
       "      <td>[0.0036688992, 0.0861908, 0.0041611576, 0.0147...</td>\n",
       "      <td>[0.0065529128, 0.06635068, 0.003924161, 0.0161...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.803588</td>\n",
       "      <td>0.651903</td>\n",
       "      <td>[[spain, GPE]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 question1  \\\n",
       "test_id                                                      \n",
       "15       What would a Trump presidency mean for current...   \n",
       "20                              Why do rockets look white?   \n",
       "21                   What's causing someone to be jealous?   \n",
       "23                                How much is 30 kV in HP?   \n",
       "34               What is the best travel website in spain?   \n",
       "\n",
       "                                                 question2  \\\n",
       "test_id                                                      \n",
       "15       How will a Trump presidency affect the student...   \n",
       "20             Why are rockets and boosters painted white?   \n",
       "21        What can I do to avoid being jealous of someone?   \n",
       "23       Where can I find a conversion chart for CC to ...   \n",
       "34                        What is the best travel website?   \n",
       "\n",
       "                                                   tokens1  \\\n",
       "test_id                                                      \n",
       "15       [what, would, a, trump, presidency, mean, for,...   \n",
       "20                         [why, do, rockets, look, white]   \n",
       "21            [what, s, causing, someone, to, be, jealous]   \n",
       "23                         [how, much, is, 30, kv, in, hp]   \n",
       "34       [what, is, the, best, travel, website, in, spain]   \n",
       "\n",
       "                                                   tokens2  \\\n",
       "test_id                                                      \n",
       "15       [how, will, a, trump, presidency, affect, the,...   \n",
       "20       [why, are, rockets, and, boosters, painted, wh...   \n",
       "21       [what, can, i, do, to, avoid, being, jealous, ...   \n",
       "23       [where, can, i, find, a, conversion, chart, fo...   \n",
       "34                  [what, is, the, best, travel, website]   \n",
       "\n",
       "         embedding_similarity  embedding_similarity_tfidf  \\\n",
       "test_id                                                     \n",
       "15                   0.894801                    0.826423   \n",
       "20                   0.871040                    0.846412   \n",
       "21                   0.948524                    0.924701   \n",
       "23                   0.804563                    0.559170   \n",
       "34                   0.962578                    0.874730   \n",
       "\n",
       "                                             vector_combo1  \\\n",
       "test_id                                                      \n",
       "15       [0.0011587327, 0.009684816, 0.001439825, 0.000...   \n",
       "20       [0.06817112, -5.824842e-05, 0.106069826, 0.044...   \n",
       "21       [0.010460816, 0.029607525, 0.08545068, 0.00066...   \n",
       "23       [-0.0070741503, 0.16010112, -0.04363538, 0.002...   \n",
       "34       [0.0036688992, 0.0861908, 0.0041611576, 0.0147...   \n",
       "\n",
       "                                             vector_combo2  firstWordSame  \\\n",
       "test_id                                                                     \n",
       "15       [0.00010098619, 0.0010915771, 0.020145686, 0.0...              0   \n",
       "20       [0.11919739, 0.0032099667, 0.10251035, 0.04612...              1   \n",
       "21       [0.06137621, -0.0012372601, 0.070046395, 0.002...              1   \n",
       "23       [0.015298527, 0.22946647, -0.00915852, 0.01152...              0   \n",
       "34       [0.0065529128, 0.06635068, 0.003924161, 0.0161...              1   \n",
       "\n",
       "         lastWordSame      ...       what  when  where  how  who  \\\n",
       "test_id                    ...                                     \n",
       "15                  0      ...          0     1      1    0    1   \n",
       "20                  1      ...          1     1      1    1    1   \n",
       "21                  0      ...          1     1      1    1    1   \n",
       "23                  0      ...          1     1      0    0    1   \n",
       "34                  0      ...          1     1      1    1    1   \n",
       "\n",
       "         embedding_similarity_max  embedding_similarity_max_tfidf  \\\n",
       "test_id                                                             \n",
       "15                       0.520753                        0.397651   \n",
       "20                       0.743635                        0.737955   \n",
       "21                       0.634457                        0.749039   \n",
       "23                       0.411248                        0.259039   \n",
       "34                       0.803588                        0.651903   \n",
       "\n",
       "                                  entities1  \\\n",
       "test_id                                       \n",
       "15                           [[Trump, ORG]]   \n",
       "20                                       []   \n",
       "21                                       []   \n",
       "23       [[30 kV, QUANTITY], [HP, PRODUCT]]   \n",
       "34                           [[spain, GPE]]   \n",
       "\n",
       "                                    entities2  entity_score  \n",
       "test_id                                                      \n",
       "15       [[Trump, ORG], [US, GPE], [US, GPE]]           0.5  \n",
       "20                                         []           1.0  \n",
       "21                                         []           1.0  \n",
       "23                                [[CC, ORG]]           0.0  \n",
       "34                                         []           0.0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['question1', 'question2', 'tokens1', 'tokens2', 'embedding_similarity',\n",
      "       'embedding_similarity_tfidf', 'vector_combo1', 'vector_combo2',\n",
      "       'firstWordSame', 'lastWordSame', 'firstWordSame_fuzzy',\n",
      "       'lastWordSame_fuzzy', 'word_ratio', 'char_ratio', 'math_similarity',\n",
      "       'token_similarity', 'token_similarity_idf', 'cross_correlation',\n",
      "       'cross_correlation_idf', 'q1_degree', 'q2_degree', 'intersection_count',\n",
      "       '2grams', '3grams', '4grams', 'edit_score', 'jaro_score', 'wl_ratio',\n",
      "       'stop_ratio', 'UC_ratio', 'why', 'what', 'when', 'where', 'how', 'who',\n",
      "       'embedding_similarity_max', 'embedding_similarity_max_tfidf'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropCols = [\"vector1\",\"vector_tfidf1\",\"vector_max1\",\"vector_max_tfidf1\",\"vector2\",\"vector_tfidf2\",\"vector_max2\",\"vector_max_tfidf2\",\"vector_combo1\",\"vector_combo2\"]\n",
    "for dropCol in dropCols:\n",
    "    if dropCol in list(df_train.columns):\n",
    "        df_train = df_train.drop(dropCol,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saveOBJ(df_train,\"gen_data/df_train\")\n",
    "saveOBJ(df_train,\"gen_data/df_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairs to omit from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3306\n",
      "13016\n",
      "47056\n",
      "96725\n",
      "104101\n",
      "134403\n",
      "190570\n",
      "208485\n",
      "213220\n",
      "226925\n",
      "273065\n",
      "301583\n",
      "384293\n",
      "402423\n"
     ]
    }
   ],
   "source": [
    "for index, row in df_train.iterrows():\n",
    "    if row[\"tokens1\"] == [] or row[\"tokens2\"] == []:\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairs to omit from testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20072\n",
      "20794\n",
      "189659\n",
      "254161\n"
     ]
    }
   ],
   "source": [
    "for index, row in df_train.iterrows():\n",
    "    if row[\"tokens1\"] == [] or row[\"tokens2\"] == []:\n",
    "        print(index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
